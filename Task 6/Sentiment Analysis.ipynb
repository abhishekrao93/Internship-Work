{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "from string import digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Activation, Dense , Dropout,Bidirectional, GlobalMaxPool1D , LSTM\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout,Conv1D,GlobalMaxPooling1D\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "forumPosts = pd.read_excel('stanfordMOOCForumPostsSet.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Opinion(1/0)</th>\n",
       "      <th>Question(1/0)</th>\n",
       "      <th>Answer(1/0)</th>\n",
       "      <th>Sentiment(1-7)</th>\n",
       "      <th>Confusion(1-7)</th>\n",
       "      <th>Urgency(1-7)</th>\n",
       "      <th>CourseType</th>\n",
       "      <th>forum_post_id</th>\n",
       "      <th>course_display_name</th>\n",
       "      <th>forum_uid</th>\n",
       "      <th>created_at</th>\n",
       "      <th>post_type</th>\n",
       "      <th>anonymous</th>\n",
       "      <th>anonymous_to_peers</th>\n",
       "      <th>up_count</th>\n",
       "      <th>comment_thread_id</th>\n",
       "      <th>reads</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Interesting! How often we say those things to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Education</td>\n",
       "      <td>5225177f2c501f0a00000015</td>\n",
       "      <td>Education/EDUC115N/How_to_Learn_Math</td>\n",
       "      <td>30CADB93E6DE4711193D7BD05F2AE95C</td>\n",
       "      <td>2013-09-02 22:55:59</td>\n",
       "      <td>Comment</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5221a8262cfae31200000001</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is \\Algebra as a Math Game\\\" or are you j...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Education</td>\n",
       "      <td>5207d0e9935dfc0e0000005e</td>\n",
       "      <td>Education/EDUC115N/How_to_Learn_Math</td>\n",
       "      <td>37D8FAEE7D0B94B6CFC57D98FD3D0BA5</td>\n",
       "      <td>2013-08-11 17:59:05</td>\n",
       "      <td>Comment</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>520663839df35b0a00000043</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I like the idea of my kids principal who says ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Education</td>\n",
       "      <td>52052c82d01fec0a00000071</td>\n",
       "      <td>Education/EDUC115N/How_to_Learn_Math</td>\n",
       "      <td>CC11480215042B3EB6E5905EAB13B733</td>\n",
       "      <td>2013-08-09 17:53:06</td>\n",
       "      <td>Comment</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51e59415e339d716000001a6</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From their responses, it seems the students re...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Education</td>\n",
       "      <td>5240a45e067ebf1200000008</td>\n",
       "      <td>Education/EDUC115N/How_to_Learn_Math</td>\n",
       "      <td>C717F838D10E8256D7C88B33C43623F1</td>\n",
       "      <td>2013-09-23 20:28:14</td>\n",
       "      <td>CommentThread</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The boys loved math, because \\there is freedom...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Education</td>\n",
       "      <td>5212c5e2dd10251500000062</td>\n",
       "      <td>Education/EDUC115N/How_to_Learn_Math</td>\n",
       "      <td>F83887D68EA48964687C6441782CDD0E</td>\n",
       "      <td>2013-08-20 01:26:58</td>\n",
       "      <td>CommentThread</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Opinion(1/0)  \\\n",
       "0  Interesting! How often we say those things to ...             1   \n",
       "1  What is \\Algebra as a Math Game\\\" or are you j...             0   \n",
       "2  I like the idea of my kids principal who says ...             1   \n",
       "3  From their responses, it seems the students re...             1   \n",
       "4  The boys loved math, because \\there is freedom...             1   \n",
       "\n",
       "   Question(1/0)  Answer(1/0)  Sentiment(1-7)  Confusion(1-7)  Urgency(1-7)  \\\n",
       "0              0            0             6.5             2.0           1.5   \n",
       "1              1            0             4.0             5.0           3.5   \n",
       "2              0            0             5.5             3.0           2.5   \n",
       "3              0            0             6.0             3.0           2.5   \n",
       "4              0            0             7.0             2.0           3.0   \n",
       "\n",
       "  CourseType             forum_post_id                   course_display_name  \\\n",
       "0  Education  5225177f2c501f0a00000015  Education/EDUC115N/How_to_Learn_Math   \n",
       "1  Education  5207d0e9935dfc0e0000005e  Education/EDUC115N/How_to_Learn_Math   \n",
       "2  Education  52052c82d01fec0a00000071  Education/EDUC115N/How_to_Learn_Math   \n",
       "3  Education  5240a45e067ebf1200000008  Education/EDUC115N/How_to_Learn_Math   \n",
       "4  Education  5212c5e2dd10251500000062  Education/EDUC115N/How_to_Learn_Math   \n",
       "\n",
       "                          forum_uid          created_at      post_type  \\\n",
       "0  30CADB93E6DE4711193D7BD05F2AE95C 2013-09-02 22:55:59        Comment   \n",
       "1  37D8FAEE7D0B94B6CFC57D98FD3D0BA5 2013-08-11 17:59:05        Comment   \n",
       "2  CC11480215042B3EB6E5905EAB13B733 2013-08-09 17:53:06        Comment   \n",
       "3  C717F838D10E8256D7C88B33C43623F1 2013-09-23 20:28:14  CommentThread   \n",
       "4  F83887D68EA48964687C6441782CDD0E 2013-08-20 01:26:58  CommentThread   \n",
       "\n",
       "   anonymous  anonymous_to_peers  up_count         comment_thread_id  reads  \n",
       "0        0.0                 0.0       0.0  5221a8262cfae31200000001   41.0  \n",
       "1        0.0                 0.0       0.0  520663839df35b0a00000043   55.0  \n",
       "2        0.0                 0.0       0.0  51e59415e339d716000001a6   25.0  \n",
       "3        0.0                 0.0       0.0                      None    0.0  \n",
       "4        0.0                 0.0       0.0                      None    3.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forumPosts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below Cell contains all the Functions used in the Notebook which help us in pre processing the Data.\n",
    "\n",
    "* remove_punct : This function is used to remove all the punctuations from the text\n",
    "* tokenization : This function is used to split longer strings of data into smaller strings\n",
    "* load_data : In this fucntion, we are converting the data which we obtained into readable format, in this case into dataframes. We also remove jargon values from the text such as start, end and user.\n",
    "* stemming : We use stemming to remove the affixes from a word and obtain the root word\n",
    "* lemmatizer : We use lemmatization to capture canonical forms based on a word's lemma. Eg : better â†’ good\n",
    "* convert_emojis : We use the convert emojis fucntion to convert the emojis into the their meaning. eg : a sushi emoji will be changed to the word sushi.\n",
    "* convert_emoticons : We use the convert emojis fucntion to convert the emojis into the their meaning. eg : a happy emoji will be changed to the text happy.\n",
    "* preprocessing : The preprocessing function is used to preprocess the text. In this function we call the others functions too which will help us in preprocessing the data. We remove punctuations, we remove stop words, emojis and emoticons. We also sem and lemmatize the data. we change the data into lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "\n",
    "# Converting emojis to words\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "\n",
    "# Converting emoticons to words    \n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "def preprocessing(df):    \n",
    "    df= df.apply(lambda x: convert_emojis(x))\n",
    "    df= df.apply(lambda x: convert_emoticons(x))\n",
    "    df = df.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    stop = stopwords.words('english')\n",
    "    df = df.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    df = df.apply(lambda x: remove_punct(x))\n",
    "    df = df.apply(lambda x: tokenization(x.lower()))\n",
    "    df = df.apply(lambda x: stemming(x))\n",
    "    df = df.apply(lambda x: lemmatizer(x))\n",
    "    \n",
    "    for i in range(0, len(df)):\n",
    "        processed_feature = re.sub(r'\\W', ' ', str(df[i]))\n",
    "        processed_feature = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "        processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "        processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "        df[i] = processed_feature.lower()\n",
    "    return df\n",
    "\n",
    "\n",
    "def int_to_string(sentiment):\n",
    "    if sentiment in [1,1.5,2,2.5,3]:\n",
    "        return 'Negative'\n",
    "    elif sentiment in [3.5,4,4.5]:\n",
    "        return 'Neutral'\n",
    "    elif sentiment in [5,5.5,6,6.5,7]:\n",
    "        return 'Positive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### While preprocessing the texts, we were facing issues because all the forumPosts['Text'] values were not in the string format and were in Int and Float format too. \n",
    "\n",
    "#### Hence, we converted all the values to the String format and then proceeded with preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of Values which are not string type\n",
      "11157\n",
      "18312\n",
      "19732\n",
      "23525\n",
      "24285\n",
      "27323\n"
     ]
    }
   ],
   "source": [
    "print(\"Index of Values which are not string type\")\n",
    "for i in range(0, len(forumPosts)):\n",
    "    #print(type(forumPosts['Text'][i]))\n",
    "    \n",
    "    if type(forumPosts['Text'][i])!=str:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(forumPosts)):\n",
    "    if type(forumPosts['Text'][i])!=str:\n",
    "        forumPosts['Text'][i] = str(forumPosts['Text'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of Values which are not string type\n"
     ]
    }
   ],
   "source": [
    "print(\"Index of Values which are not string type\")\n",
    "for i in range(0, len(forumPosts)):\n",
    "    #print(type(forumPosts['Text'][i]))\n",
    "    if type(forumPosts['Text'][i])!=str:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "forumPosts[\"Text\"] = preprocessing(forumPosts[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(forumPosts[\"Text\"][3])\n",
    "forumPosts.Text.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "forumPosts[\"Sentiments\"] = forumPosts[\"Sentiment(1-7)\"].apply(int_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Neutral', 'Negative'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forumPosts[\"Sentiments\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = forumPosts.drop('Sentiment(1-7)', axis=1)\n",
    "y = forumPosts['Sentiments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% training and 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2,random_state=69) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23683, 18)\n",
      "(5921, 18)\n",
      "(23683,)\n",
      "(5921,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(max_features=500,min_df=5, max_df=0.8,ngram_range=(1,3))\n",
    "tfidf_vect.fit(X_train[\"Text\"])\n",
    "X_train_tfidf = tfidf_vect.transform(X_train[\"Text\"])\n",
    "X_test_tfidf = tfidf_vect.transform(X_test[\"Text\"])\n",
    "#y_train_dummy = to_categorical(y_train)\n",
    "#y_test_dummy = to_categorical(y_test)\n",
    "y_train_dummy = pd.get_dummies(y_train).values\n",
    "y_test_dummy = pd.get_dummies(y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23683, 500)\n",
      "(5921, 500)\n",
      "(23683, 3)\n",
      "(5921, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)\n",
    "print(y_train_dummy.shape)\n",
    "print(y_test_dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a CNN Model\n",
    "\n",
    "* Dense is a standard layer type that is used in many cases for neural networks.\n",
    "* Relu that is rectified linear activation function returns the value provided as input directly, when training a neural network.\n",
    "* add function is used to add layers to our model.\n",
    "* Sequential model is used as the layers are stacked sequentially that is input and output layer with their respective shapes.\n",
    "* As the output layer is a multiclass classification problem \"softmax\" has been used as output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=5000\n",
    "maxlen = X_train_tfidf.shape[1]\n",
    "\n",
    "embedding_size=32\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_size, input_length=maxlen))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 496, 128)          20608     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 181,931\n",
      "Trainable params: 181,931\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "             optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23683 samples, validate on 5921 samples\n",
      "Epoch 1/10\n",
      "23683/23683 [==============================] - 32s 1ms/step - loss: 0.7492 - accuracy: 0.7578 - val_loss: 0.6663 - val_accuracy: 0.7874\n",
      "Epoch 2/10\n",
      "23683/23683 [==============================] - 28s 1ms/step - loss: 0.6767 - accuracy: 0.7859 - val_loss: 0.6659 - val_accuracy: 0.7874\n",
      "Epoch 3/10\n",
      "23683/23683 [==============================] - 28s 1ms/step - loss: 0.6682 - accuracy: 0.7860 - val_loss: 0.6609 - val_accuracy: 0.7874\n",
      "Epoch 4/10\n",
      "23683/23683 [==============================] - 28s 1ms/step - loss: 0.6628 - accuracy: 0.7860 - val_loss: 0.6552 - val_accuracy: 0.7874\n",
      "Epoch 5/10\n",
      "23683/23683 [==============================] - 28s 1ms/step - loss: 0.6647 - accuracy: 0.7860 - val_loss: 0.6564 - val_accuracy: 0.7874\n",
      "Epoch 6/10\n",
      "23683/23683 [==============================] - 28s 1ms/step - loss: 0.6593 - accuracy: 0.7860 - val_loss: 0.6542 - val_accuracy: 0.7874\n",
      "Epoch 7/10\n",
      "23683/23683 [==============================] - 28s 1ms/step - loss: 0.6583 - accuracy: 0.7860 - val_loss: 0.6615 - val_accuracy: 0.7874\n",
      "Epoch 8/10\n",
      "23683/23683 [==============================] - 30s 1ms/step - loss: 0.6573 - accuracy: 0.7860 - val_loss: 0.6637 - val_accuracy: 0.7874\n",
      "Epoch 9/10\n",
      "23683/23683 [==============================] - 33s 1ms/step - loss: 0.6549 - accuracy: 0.7860 - val_loss: 0.6524 - val_accuracy: 0.7874\n",
      "Epoch 10/10\n",
      "23683/23683 [==============================] - 35s 1ms/step - loss: 0.6550 - accuracy: 0.7860 - val_loss: 0.6606 - val_accuracy: 0.7874\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_tfidf, y_train_dummy,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_tfidf, y_test_dummy),\n",
    "                    batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       354\n",
      "           1       0.79      1.00      0.88      4662\n",
      "           2       0.00      0.00      0.00       905\n",
      "\n",
      "    accuracy                           0.79      5921\n",
      "   macro avg       0.26      0.33      0.29      5921\n",
      "weighted avg       0.62      0.79      0.69      5921\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred_tfidf = model.predict(X_test_tfidf)\n",
    "y_test_dummy_new = np.argmax(y_test_dummy, axis=1)\n",
    "y_pred_tfidf = np.argmax(y_pred_tfidf, axis=1)\n",
    "print(classification_report(y_test_dummy_new,y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob_tfidf = model.predict_proba(X_test_tfidf)\n",
    "auc_score = roc_auc_score(y_test_dummy_new, pred_prob_tfidf, multi_class=\"ovo\",\n",
    "                                  average=\"macro\")\n",
    "print('ROC-AUC = %.2f'% auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model Using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_function(n_estimators ,learning_rate):\n",
    "     params = {'n_estimators': int(n_estimators),\n",
    "              'learning_rate':learning_rate,\n",
    "              'subsample': 0.8,\n",
    "              'eta': 0.1,\n",
    "              'eval_metric': 'rmse',\n",
    "             'max_depth':int(4),\n",
    "             'min_child_weight':6,\n",
    "             'gamma':0,\n",
    "             'subsample':0.8,\n",
    "             'colsample_bytree':0.8,\n",
    "             'reg_alpha':0.005,\n",
    "             'objective': 'logistic',\n",
    "             'nthread':4,\n",
    "             'seed':27,\n",
    "             'eta':0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "bayes = BayesianOptimization(tuning_function, {'n_estimators':(500,1000),\n",
    "                                            'learning_rate':(0,0.5)\n",
    "                                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bayes.maximize(n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = bayes.max['params']\n",
    "params['n_estimators']= int(params['n_estimators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model_xgboost = xgb.XGBClassifier(**params).fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_HT = model_xgboost.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.51      0.11      0.18       354\n",
      "     Neutral       0.85      0.95      0.90      4662\n",
      "    Positive       0.68      0.46      0.55       905\n",
      "\n",
      "    accuracy                           0.83      5921\n",
      "   macro avg       0.68      0.51      0.54      5921\n",
      "weighted avg       0.80      0.83      0.80      5921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred_HT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC = 0.84\n"
     ]
    }
   ],
   "source": [
    "pred_prob_hyper = model_xgboost.predict_proba(X_test_tfidf)\n",
    "auc_score = roc_auc_score(y_test, pred_prob_hyper, multi_class=\"ovo\",\n",
    "                                  average=\"macro\")\n",
    "print('ROC-AUC = %.2f'% auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.83\n"
     ]
    }
   ],
   "source": [
    "cm_hp = confusion_matrix(y_test, y_pred_HT)\n",
    "accuracy = cm_hp.diagonal().sum()/cm_hp.sum()\n",
    "print('Accuracy = %.2f'% accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Using RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 1000)              501000    \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 3)                 3003      \n",
      "=================================================================\n",
      "Total params: 4,508,003\n",
      "Trainable params: 4,508,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=500, activation=\"relu\", kernel_regularizer=<keras.reg..., units=1000)`\n",
      "  \n",
      "C:\\Users\\abhis\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=1000, activation=\"relu\", kernel_regularizer=<keras.reg..., units=1000)`\n",
      "  import sys\n",
      "C:\\Users\\abhis\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=1000, activation=\"relu\", kernel_regularizer=<keras.reg..., units=1000)`\n",
      "  \n",
      "C:\\Users\\abhis\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=1000, activation=\"relu\", kernel_regularizer=<keras.reg..., units=1000)`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\abhis\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=1000, activation=\"relu\", kernel_regularizer=<keras.reg..., units=1000)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "embedding_size=32\n",
    "vocabulary_size = 5000\n",
    "maxlen = X_train_tfidf.shape[1]\n",
    "model_RNN = Sequential()\n",
    "model_RNN.add(layers.Dense(output_dim=1000, input_dim=maxlen, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model_RNN.add(layers.Dense(output_dim=1000, input_dim=1000, activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model_RNN.add(layers.Dense(output_dim=1000, input_dim=1000, activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model_RNN.add(layers.Dense(output_dim=1000, input_dim=1000, activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model_RNN.add(layers.Dense(output_dim=1000, input_dim=1000, activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model_RNN.add(layers.Dense(3, activation='softmax',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "print(model_RNN.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RNN.compile(loss='categorical_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23683, 500)\n",
      "(5921, 500)\n",
      "(23683, 3)\n",
      "(5921, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)\n",
    "print(y_train_dummy.shape)\n",
    "print(y_test_dummy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23683 samples, validate on 5921 samples\n",
      "Epoch 1/3\n",
      "23683/23683 [==============================] - 45s 2ms/step - loss: 0.6832 - accuracy: 0.8075 - val_loss: 0.5461 - val_accuracy: 0.8238\n",
      "Epoch 2/3\n",
      "23683/23683 [==============================] - 45s 2ms/step - loss: 0.5044 - accuracy: 0.8323 - val_loss: 0.5315 - val_accuracy: 0.8222\n",
      "Epoch 3/3\n",
      "23683/23683 [==============================] - 44s 2ms/step - loss: 0.4455 - accuracy: 0.8507 - val_loss: 0.5638 - val_accuracy: 0.8110\n"
     ]
    }
   ],
   "source": [
    "history = model_RNN.fit(X_train_tfidf, y_train_dummy,\n",
    "                    epochs=3,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_tfidf, y_test_dummy),\n",
    "                    batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.10      0.16       354\n",
      "           1       0.87      0.90      0.88      4662\n",
      "           2       0.55      0.62      0.58       905\n",
      "\n",
      "    accuracy                           0.81      5921\n",
      "   macro avg       0.66      0.54      0.54      5921\n",
      "weighted avg       0.80      0.81      0.80      5921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_rnn = model_RNN.predict(X_test_tfidf)\n",
    "y_test_dummy_new = np.argmax(y_test_dummy, axis=1)\n",
    "y_pred_rnn = np.argmax(y_pred_rnn, axis=1)\n",
    "print(classification_report(y_test_dummy_new,y_pred_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC = 0.84\n"
     ]
    }
   ],
   "source": [
    "pred_prob_tfidf = model_RNN.predict_proba(X_test_tfidf)\n",
    "auc_score = roc_auc_score(y_test_dummy_new, pred_prob_tfidf, multi_class=\"ovo\",\n",
    "                                  average=\"macro\")\n",
    "print('ROC-AUC = %.2f'% auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' interest often say thing other without realli understand say must power experi excel ',\n",
       " ' algebra math game say creat game incorpor algebra ',\n",
       " ' like idea kid princip say smart mean easi smart mean work hard incorpor idea make mistak work hard ',\n",
       " ' respons seem student realli like power felt free solv math way want use academ languag like decompos number friendli number abl explain mean ',\n",
       " ' boy love math there freedom anyth great way see math number think student realiz math could taught differ thought that learn third grader need know go fourth grade thi inspir happyfaceorsmiley ']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_lines = list()\n",
    "texts = forumPosts['Text'].values.tolist()\n",
    "\n",
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts:\n",
    "    tokens=word_tokenize(text)\n",
    "    tokens=[w.lower() for w in tokens]\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not  w in stop_words]\n",
    "    text_lines.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29604"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 32838\n"
     ]
    }
   ],
   "source": [
    "gensim_model = gensim.models.Word2Vec(sentences=text_lines,\n",
    "                              min_count=1,\n",
    "                     window=5,\n",
    "                     size=100, \n",
    "                     workers=4)\n",
    "words = list(gensim_model.wv.vocab)\n",
    "print('Vocab Size: %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['interest',\n",
       " 'often',\n",
       " 'say',\n",
       " 'thing',\n",
       " 'without',\n",
       " 'realli',\n",
       " 'understand',\n",
       " 'must',\n",
       " 'power',\n",
       " 'experi']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amaz', 0.7977184057235718),\n",
       " ('love', 0.7947539687156677),\n",
       " ('intrigu', 0.7735730409622192),\n",
       " ('experienti', 0.7690808773040771),\n",
       " ('enjoy', 0.7627804279327393),\n",
       " ('conlus', 0.7576484680175781),\n",
       " ('sensemak', 0.7575889825820923),\n",
       " ('httpsskepticalannoyedundecideduneasyorhesitantclassstanfordeducoursesmedicinesciwritefallzipredaccoursewarecdcfaezipredacbfadeefezipredacfzipredaczipredacecfzipredacbbccfadzipredacfdc',\n",
       "  0.7543987035751343),\n",
       " ('wonder', 0.7533406019210815),\n",
       " ('excit', 0.749046802520752)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model.wv.most_similar('interest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.9462562 , -0.00630306,  0.7648442 ,  0.6192774 , -0.5520163 ,\n",
       "       -0.7077054 , -1.0221578 ,  0.6327603 ,  0.82904   , -0.11447569,\n",
       "       -0.16207682,  0.43830568,  0.05797423, -0.04881771,  1.3803277 ,\n",
       "        1.4259303 , -0.07638354,  0.07553734, -0.46576515, -0.3949533 ,\n",
       "        1.2401513 , -1.6041169 , -0.38952804,  0.3510483 ,  0.42860246,\n",
       "        0.7320395 ,  0.91205055,  0.307704  , -0.11935808,  0.6120276 ,\n",
       "       -0.29160148,  1.3145282 ,  0.70989317,  0.47141853, -0.40127844,\n",
       "        1.2013286 ,  0.1024786 , -1.634186  ,  1.7706367 ,  0.7219945 ,\n",
       "       -0.31511906,  0.14647202, -0.36780256,  0.6501047 ,  0.2890127 ,\n",
       "        0.36015418,  1.0926875 ,  0.328415  , -0.02660573,  1.0674133 ,\n",
       "        0.8810085 ,  0.31367183, -0.74809986, -0.3545218 ,  0.33769673,\n",
       "        0.5710678 , -1.7280122 , -0.10289101, -0.6749679 ,  0.08173798,\n",
       "       -1.0831168 , -1.0960724 ,  0.25910637, -0.1612196 ,  0.6253032 ,\n",
       "       -0.91432726,  0.59945095, -1.1646962 ,  0.74066377, -0.88825697,\n",
       "       -0.05517545, -0.22045065,  0.22504137, -0.82028264, -0.54150164,\n",
       "       -0.7622994 ,  0.5375871 ,  0.47034454,  1.3836775 , -1.3112981 ,\n",
       "       -1.0471543 ,  1.8679805 ,  0.02679228,  1.9875376 , -0.34133694,\n",
       "        1.1486794 ,  0.6283174 , -0.03081493,  0.10593379,  0.49201885,\n",
       "        0.43747404,  1.403587  ,  1.0852724 , -1.492904  ,  0.5582293 ,\n",
       "       -0.83466953, -0.88483477, -0.5616205 , -0.05565573,  0.8126155 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model['enjoy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The parameters:\n",
    "\n",
    "* min_count = int - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
    "* window = int - The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\n",
    "* size = int - Dimensionality of the feature vectors. - (50, 300)\n",
    "* sample = float - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)\n",
    "* alpha = float - The initial learning rate - (0.01, 0.05)\n",
    "* min_alpha = float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
    "* negative = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
    "* workers = int - Use these many worker threads to train the model (=faster training with multicore machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.txt'\n",
    "gensim_model.wv.save_word2vec_format(filename,binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_index = {}\n",
    "\n",
    "f = open(os.path.join('','model.txt'),encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embedding_index[word] = coefs\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique token 32838\n",
      "(29604, 268)\n",
      "(29604,)\n"
     ]
    }
   ],
   "source": [
    "tokenized_object = Tokenizer()\n",
    "tokenized_object.fit_on_texts(text_lines)\n",
    "seq = tokenized_object.texts_to_sequences(text_lines)\n",
    "\n",
    "#pad sequence\n",
    "word_index = tokenized_object.word_index\n",
    "print('Unique token %d' % len(word_index))\n",
    "text_pad = pad_sequences(seq,maxlen=268)\n",
    "sentiment = forumPosts['Sentiments']\n",
    "print(text_pad.shape)\n",
    "print(sentiment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words,100))\n",
    "\n",
    "for word,i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32839\n"
     ]
    }
   ],
   "source": [
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(text_pad,sentiment, test_size=0.2,random_state=69) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23683, 3)\n",
      "(5921, 3)\n"
     ]
    }
   ],
   "source": [
    "y_train_word2vec_dummy = pd.get_dummies(y_train_word2vec).values\n",
    "y_test_word2vec_dummy = pd.get_dummies(y_test_word2vec).values\n",
    "\n",
    "print(y_train_word2vec_dummy.shape)\n",
    "print(y_test_word2vec_dummy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=50000\n",
    "maxlen = X_train_word2vec.shape[1]\n",
    "\n",
    "embedding_size=64\n",
    "model_w2v=Sequential()\n",
    "model_w2v.add(Embedding(vocab_size, embedding_size, input_length=maxlen))\n",
    "model_w2v.add(Dropout(0.25))\n",
    "model_w2v.add(Conv1D(128, 5, activation='relu'))\n",
    "model_w2v.add(GlobalMaxPooling1D())\n",
    "model_w2v.add(Dense(10, activation='relu'))\n",
    "model_w2v.add(Dropout(0.25))\n",
    "\n",
    "model_w2v.add(Dense(3, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 268, 64)           3200000   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 268, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 264, 128)          41088     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 3,242,411\n",
      "Trainable params: 3,242,411\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_w2v.compile(loss='categorical_crossentropy', \n",
    "             optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model_w2v.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23683 samples, validate on 5921 samples\n",
      "Epoch 1/10\n",
      "23683/23683 [==============================] - 36s 2ms/step - loss: 0.7537 - accuracy: 0.7724 - val_loss: 0.6744 - val_accuracy: 0.7874\n",
      "Epoch 2/10\n",
      "23683/23683 [==============================] - 37s 2ms/step - loss: 0.6448 - accuracy: 0.7860 - val_loss: 0.6067 - val_accuracy: 0.7874\n",
      "Epoch 3/10\n",
      "23683/23683 [==============================] - 38s 2ms/step - loss: 0.5905 - accuracy: 0.7860 - val_loss: 0.5672 - val_accuracy: 0.7874\n",
      "Epoch 4/10\n",
      "23683/23683 [==============================] - 37s 2ms/step - loss: 0.5543 - accuracy: 0.7860 - val_loss: 0.5492 - val_accuracy: 0.7874\n",
      "Epoch 5/10\n",
      "23683/23683 [==============================] - 38s 2ms/step - loss: 0.5145 - accuracy: 0.7860 - val_loss: 0.5389 - val_accuracy: 0.7874\n",
      "Epoch 6/10\n",
      "23683/23683 [==============================] - 34s 1ms/step - loss: 0.4780 - accuracy: 0.7854 - val_loss: 0.5363 - val_accuracy: 0.7874\n",
      "Epoch 7/10\n",
      "23683/23683 [==============================] - 34s 1ms/step - loss: 0.4430 - accuracy: 0.7834 - val_loss: 0.5433 - val_accuracy: 0.7874\n",
      "Epoch 8/10\n",
      "23683/23683 [==============================] - 40s 2ms/step - loss: 0.4104 - accuracy: 0.8458 - val_loss: 0.5569 - val_accuracy: 0.8049\n",
      "Epoch 9/10\n",
      "23683/23683 [==============================] - 33s 1ms/step - loss: 0.3845 - accuracy: 0.8626 - val_loss: 0.5803 - val_accuracy: 0.8100\n",
      "Epoch 10/10\n",
      "23683/23683 [==============================] - 33s 1ms/step - loss: 0.3639 - accuracy: 0.8654 - val_loss: 0.6015 - val_accuracy: 0.8041\n"
     ]
    }
   ],
   "source": [
    "history = model_w2v.fit(X_train_word2vec, y_train_word2vec_dummy,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_word2vec, y_test_word2vec_dummy),\n",
    "                    batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       354\n",
      "           1       0.85      0.93      0.89      4662\n",
      "           2       0.51      0.48      0.49       905\n",
      "\n",
      "    accuracy                           0.80      5921\n",
      "   macro avg       0.45      0.47      0.46      5921\n",
      "weighted avg       0.75      0.80      0.78      5921\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred_word2vec = model_w2v.predict(X_test_word2vec)\n",
    "y_test_word2vec_dummy = np.argmax(y_test_word2vec_dummy, axis=1)\n",
    "y_pred_word2vec = np.argmax(y_pred_word2vec, axis=1)\n",
    "print(classification_report(y_test_word2vec_dummy,y_pred_word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC = 0.71\n"
     ]
    }
   ],
   "source": [
    "pred_prob_word2vec = model_w2v.predict_proba(X_test_word2vec)\n",
    "auc_score_w2v = roc_auc_score(y_test_word2vec_dummy, pred_prob_word2vec, multi_class=\"ovo\",\n",
    "                                  average=\"macro\")\n",
    "print('ROC-AUC = %.2f'% auc_score_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From the above 2 models we can see that the model built using TFIDF(acc=0.79, roc-auc= 0.50) has accuracy and roc-auc values lower than the accuracy and roc-auc value of the model built with Word2Vec(acc=0.82, roc-auc= 0.83). \n",
    "\n",
    "# The accuracy and roc-auc for RNN(acc=0.81, roc-auc= 0.84) and XGBoost (acc=0.83, roc-auc=0.84 )model.\n",
    "\n",
    "\n",
    "# From the above we can see that accuracy using XGboost model is the highest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
